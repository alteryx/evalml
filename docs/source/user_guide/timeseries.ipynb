{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoMLSearch for time series problems\n",
    "\n",
    "In this guide, we'll show how you can use EvalML to perform an automated search of machine learning pipelines for time series problems. Time series support is still being actively developed in EvalML so expect this page to improve over time.\n",
    "\n",
    "## But first, what is a time series?\n",
    "\n",
    "A time series is a series of measurements taken at different moments in time ([Wikipedia](https://en.wikipedia.org/wiki/Time_series)). The main difference between a time series dataset and a normal dataset is that the rows of a time series dataset are ordered with time. This relationship between the rows does not exist in non-time series datasets. In a non-time-series dataset, you can shuffle the rows and the dataset still has the same meaning. If you shuffle the rows of a time series dataset, the relationship between the rows is completely different!\n",
    "\n",
    "## What does AutoMLSearch for time series do?\n",
    "In a machine learning setting, we are usually interested in using past values of the time series to predict future values. That is what EvalML's time series functionality is built to do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "In this guide, we work with daily minimum temperature recordings from Melbourne, Austrailia from the beginning of 1981 to end of 1990.\n",
    "\n",
    "We start by loading the temperature data into two splits. The first split will be a training split consisting of data from 1981 to end of 1989. This is the data we'll use to find the best pipeline with AutoML. The second split will be a testing split consisting of data from 1990. This is the split we'll use to evaluate how well our pipeline generalizes on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalml.demos import load_weather\n",
    "X, y = load_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dates, test_dates = X.Date < \"1990-01-01\", X.Date >= \"1990-01-01\"\n",
    "X_train, y_train = X.ww.loc[train_dates], y.ww.loc[train_dates]\n",
    "X_test, y_test =  X.ww.loc[test_dates], y.ww.loc[test_dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Scatter(\n",
    "        x=X_train[\"Date\"],\n",
    "        y=y_train,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Temperature (C)\",\n",
    "        line=dict(color=\"#1f77b4\"),\n",
    "    )\n",
    "]\n",
    "# Let plotly pick the best date format.\n",
    "layout = go.Layout(\n",
    "    title={\"text\": \"Min Daily Temperature, Melbourne 1980-1989\"},\n",
    "    xaxis={\"title\": \"Time\"},\n",
    "    yaxis={\"title\": \"Temperature (C)\"},\n",
    ")\n",
    "\n",
    "go.Figure(data=data, layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running AutoMLSearch\n",
    "\n",
    "`AutoMLSearch` for time series problems works very similarly to the other problem types with the exception that users need to pass in a new parameter called `problem_configuration`.\n",
    "\n",
    "The `problem_configuration` is a dictionary specifying the following values:\n",
    "\n",
    "* **forecast_horizon**: The number of time periods we are trying to forecast. In this example, we're interested in predicting weather for the next 7 days, so the value is 7.\n",
    "\n",
    "* **gap**: The number of time periods between the end of the training set and the start of the test set. For example, in our case we are interested in predicting the weather for the next 7 days with the data as it is \"today\", so the gap is 0. However, if we had to predict the weather for next Monday-Sunday with the data as it was on the previous Friday, the gap would be 2 (Saturday and Sunday separate Monday from Friday). It is important to select a value that matches the realistic delay between the forecast date and the most recently avaliable data that can be used to make that forecast.\n",
    "\n",
    "* **max_delay**: The maximum number of rows to look in the past from the current row in order to compute features. In our example, we'll say we can use the previous week's weather to predict the current week's.\n",
    "\n",
    "* **time_index**: The column of the training dataset that contains the date corresponding to each observation. Currently, this parameter is only used by some time-series specific models so in this example, we are passing in `None`.\n",
    "\n",
    "Note that the values of these parameters must be in the same units as the training/testing data.\n",
    "\n",
    "### Visualization of forecast horizon and gap\n",
    "![forecast and gap](ts_viz/ts_parameter_viz.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalml.automl import AutoMLSearch\n",
    "\n",
    "problem_config = {\"gap\": 0, \"max_delay\": 7, \"forecast_horizon\": 7, \"time_index\": \"Date\"}\n",
    "\n",
    "automl = AutoMLSearch(X_train, y_train, problem_type=\"time series regression\",\n",
    "                      max_batches=1,\n",
    "                      problem_configuration=problem_config,\n",
    "                      allowed_model_families=[\"xgboost\", \"random_forest\", \"linear_model\", \"extra_trees\",\n",
    "                                              \"decision_tree\"]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding what happened under the hood\n",
    "\n",
    "This is great, ``AutoMLSearch`` is able to find a pipeline that scores an R2 value of 0.44 compared to a baseline pipeline that is only able to score 0.07. But how did it do that? \n",
    "\n",
    "### Data Splitting\n",
    "\n",
    "EvalML uses [rolling origin cross validation](https://robjhyndman.com/hyndsight/tscv/) for time series problems. Basically, we take successive cuts of the training data while keeping the validation set size fixed. Note that the splits are not separated by ``gap`` number of units. This is because we need access to all the data to generate features for every row of the validation set. However, the feature engineering done by our pipelines respects the ``gap`` value. This is explained more in the [feature engineering section](#Feature-Engineering).\n",
    "\n",
    "![cross validation](ts_viz/cv_viz.png)\n",
    "\n",
    "### Baseline Pipeline\n",
    "\n",
    "The most naive thing we can do in a time series problem is use the most recently available observation to predict the next observation. In our example, this means we'll use the measurement from ``7`` days ago as the prediction for the current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "baseline = automl.get_pipeline(0)\n",
    "baseline.fit(X_train, y_train)\n",
    "naive_baseline_preds = baseline.predict_in_sample(X_test, y_test, objective=None,\n",
    "                                                  X_train=X_train, y_train=y_train)\n",
    "expected_preds = pd.concat([y_train.iloc[-7:], y_test]).shift(7).iloc[7:]\n",
    "pd.testing.assert_series_equal(expected_preds, naive_baseline_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "EvalML uses the values of `gap`, `forecast_horizon`, and `max_delay` to calculate a \"window\" of allowed dates that can be used for engineering the features of each row in the validation/test set. The formula for computing the bounds of the window is:\n",
    "\n",
    "<br>  \n",
    "<div align=\"center\"><b>[t - (max_delay + forecast_horizon + gap), t - (forecast_horizon + gap)]</b></div>\n",
    "\n",
    "\n",
    "As an example, this is what the features for the first five days of August would look like in our current problem:\n",
    "\n",
    "![features](ts_viz/feature_engineering_window_viz.png)\n",
    "\n",
    "The estimator then takes these features to generate predictions:\n",
    "\n",
    "![estimator predictions](ts_viz/estimator_viz.png)\n",
    "\n",
    "#### Feature engineering components for time series\n",
    "For an example of a time-series feature engineering component see [TimeSeriesFeaturizer](../autoapi/evalml/pipelines/components/index.rst#evalml.pipelines.components.TimeSeriesFeaturizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate best pipeline on test data\n",
    "\n",
    "Now that we have covered the mechanics of how EvalML runs AutoMLSearch for time series pipelines, we can compare the performance on the test set of the best pipeline found during search and the baseline pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = automl.best_pipeline\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "best_pipeline_score = pl.score(X_test, y_test, ['R2'], X_train, y_train)['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = automl.get_pipeline(0)\n",
    "baseline.fit(X_train, y_train)\n",
    "naive_baseline_score = baseline.score(X_test, y_test, ['R2'], X_train, y_train)['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_baseline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline found by AutoMLSearch has a 268% improvement over the naive forecast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.objective.calculate_percent_difference(best_pipeline_score, naive_baseline_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the predictions over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalml.model_understanding import graph_prediction_vs_actual_over_time\n",
    "\n",
    "fig = graph_prediction_vs_actual_over_time(pl, X_test, y_test, X_train, y_train, dates=X_test['Date'])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on unseen data\n",
    "\n",
    "You'll notice that in the code snippets here, we use the ``predict_in_sample`` pipeline method as opposed to the usual ``predict`` method. What's the difference?\n",
    "\n",
    "* ``predict_in_sample`` is used when the target value is known on the dates we are predicting on. This is true in cross validation. This method has an expected ``y`` parameter so that we can compute features using previous target values for all of the observations on the holdout set.\n",
    "* ``predict`` is used when the target value is not known, e.g. the test dataset. The y parameter is not expected as only the target is observed in the training set. The test dataset must be separated by ``gap`` units from the training dataset. For the moment, the test set size must be less than or equal to ``forecast_horizon``.\n",
    "\n",
    "Here is an example of these two methods in action:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict_in_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict_in_sample(X_test, y_test, objective=None, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the holdout data\n",
    "\n",
    "Before we predict on our holdout data, it is important to validate that it meets the requirements we summarized in the second point above in **Predicting on sunseen data**. We can call on `validate_holdout_datasets` in order to verify the two requirements:\n",
    "\n",
    "1. The holdout data is separated by `gap` units from the training dataset. This is determined by the `time_index` column, not the index e.g. if your datetime frequency for the column \"Date\" is 2 days with a `gap` of 3, then the holdout data must start 2 days x 3 = 6 days after the training data.\n",
    "2. The length of the holdout data must be less than or equal to the `forecast_horizon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalml.utils.gen_utils import validate_holdout_datasets\n",
    "\n",
    "# Holdout dataset has 365 observations\n",
    "validation_results = validate_holdout_datasets(X_test, X_train, problem_config)\n",
    "assert not validation_results.is_valid\n",
    "# Holdout dataset has 7 observations\n",
    "validation_results = validate_holdout_datasets(X_test.iloc[:pl.forecast_horizon], X_train, problem_config)\n",
    "assert validation_results.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict -- Test set size matches forecast horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict(X_test.iloc[:pl.forecast_horizon], objective=None, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict -- Test set size is less than forecast horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict(X_test.iloc[:pl.forecast_horizon - 2], objective=None, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict -- Test set size index starts at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.predict(X_test.iloc[:pl.forecast_horizon].reset_index(drop=True), objective=None, X_train=X_train, y_train=y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
