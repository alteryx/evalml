{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EvalML provides data checks to help guide you in achieving the highest performing model. These utility functions help deal with problems such as overfitting, abnormal data, and missing data. These data checks can be found under `evalml/data_checks`. Below we will cover examples for each available data check in EvalML, as well as the `DefaultDataChecks` used in `AutoMLSearch.search`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "Missing data or rows with `NaN` values provide many challenges for machine learning pipelines. In the worst case, many algorithms simply will not run with missing data! EvalML pipelines contain imputation [components](../user_guide/components.ipynb) to ensure that doesn't happen. Imputation works by approximating missing values with existing values. However, if a column contains a high number of missing values, a large percentage of the column would be approximated by a small percentage. This could potentially create a column without useful information for machine learning pipelines. By using the `HighlyNullDataCheck` data check, EvalML will alert you to this potential problem by returning the columns that pass the missing values threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from evalml.data_checks import HighlyNullDataCheck\n",
    "\n",
    "X = pd.DataFrame([[1, 2, 3], \n",
    "                  [0, 4, np.nan],\n",
    "                  [1, 4, np.nan],\n",
    "                  [9, 4, np.nan],\n",
    "                  [8, 6, np.nan]])\n",
    "\n",
    "null_check = HighlyNullDataCheck(pct_null_threshold=0.8)\n",
    "\n",
    "for message in null_check.validate(X):\n",
    "    print (message.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abnormal Data\n",
    "\n",
    "EvalML provides a few data checks to check for abnormal data: \n",
    "- `OutliersDataCheck`\n",
    "- `ClassImbalanceDataCheck`\n",
    "- `IDColumnsDataCheck`\n",
    "- `NoVarianceDataCheck`\n",
    "- `HighVarianceCVDataCheck`\n",
    "- `InvalidTargetDataCheck`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Variance\n",
    "\n",
    "Data with zero variance indicates that all values are identical. If a feature has zero variance, it is not likely to be a useful feature. Similarly, if the target has zero variance, there is likely something wrong. `NoVarianceDataCheck` checks if the target or any feature has only one unique value and alerts you to any such columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalml.data_checks import NoVarianceDataCheck\n",
    "\n",
    "X = pd.DataFrame([[0, 53, 1, 5],\n",
    "                  [0, 90, 3, 10],\n",
    "                  [0, 90, 18, 20]])\n",
    "y = pd.Series([1, 0, 1])\n",
    "no_variance_data_check = NoVarianceDataCheck()\n",
    "\n",
    "for message in no_variance_data_check.validate(X, y):\n",
    "    print (message.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `NaN` values count as an unique value, but `NoVarianceDataCheck` will still return a warning if there is only one unique non-`NaN` value in a given column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalml.data_checks import NoVarianceDataCheck\n",
    "\n",
    "X = pd.DataFrame([[0, np.nan, 1, 5],\n",
    "                  [10, 90, 3, 10],\n",
    "                  [0, 90, 18, 20]])\n",
    "y = pd.Series([1, 0, 1])\n",
    "\n",
    "no_variance_data_check = NoVarianceDataCheck()\n",
    "\n",
    "for message in no_variance_data_check.validate(X, y):\n",
    "    print (message.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "\n",
    "For classification problems, the distribution of examples across each class can vary. For small variations, this is normal and expected. However, when the number of examples for each class label is disproportionately biased or skewed towards a particular class (or classes), it can be difficult for machine learning models to predict well. In addition, having a low number of examples for a given class could mean that one or more of the CV folds generated for the training data could only have few or no examples from the minority class. This may cause the model need only predict the majority class correctly, resulting in a poor-performant model.\n",
    "\n",
    "`ClassImbalanceDataCheck` checks if the target labels are imbalanced beyond a specified threshold for a certain number of CV folds. It returns errors for any classes that have less samples than double the number of CV folds, and warnings for any classes that fall below the set threshold percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalml.data_checks import ClassImbalanceDataCheck\n",
    "\n",
    "X = pd.DataFrame({[[1, 2, 0, 1],\n",
    "                  [4, 1, 9, 0],\n",
    "                  [4, 4, 8, 3],\n",
    "                  [9, 2, 7, 1]]})\n",
    "y = pd.Series([0, 1, 1, 1, 1])\n",
    "class_imbalance_check = ClassImbalanceDataCheck(threshold=0.25)\n",
    "\n",
    "for message in class_imbalance_check.validate(X, y):\n",
    "    print (message.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invalid Target Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID Columns\n",
    "\n",
    "ID columns in your dataset provide little to no benefit to a machine learning pipeline as the pipeline cannot extrapolate useful information from unique identifiers. Thus, `IDColumnsDataCheck` reminds you if these columns exists. In the given example, 'user_number' and 'id' columns are both identified as potentially being unique identifiers that should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Variance Cross-Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evalml.data_checks import IDColumnsDataCheck\n",
    "\n",
    "X = pd.DataFrame([[0, 53, 6325, 5],[1, 90, 6325, 10],[2, 90, 18, 20]], columns=['user_number', 'cost', 'revenue', 'id'])\n",
    "id_col_check = IDColumnsDataCheck(id_threshold=0.9)\n",
    "\n",
    "for message in id_col_check.validate(X):\n",
    "    print (message.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "Outliers are observations that differ significantly from other observations in the same sample. Many machine learning pipelines suffer in performance if outliers are not dropped from the training set as they are not representative of the data. `OutliersDataCheck()` uses Isolation Forests to notify you if a sample can be considered an outlier.\n",
    "\n",
    "Below we generate a random dataset with some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.random.randn(100, 100)\n",
    "X = pd.DataFrame(data=data)\n",
    "\n",
    "# generate some outliers in rows 3, 25, 55, and 72\n",
    "X.iloc[3, :] = pd.Series(np.random.randn(100) * 10)\n",
    "X.iloc[25, :] = pd.Series(np.random.randn(100) * 20)\n",
    "X.iloc[55, :] = pd.Series(np.random.randn(100) * 100)\n",
    "X.iloc[72, :] = pd.Series(np.random.randn(100) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then utilize `OutliersDataCheck()` to rediscover these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evalml.data_checks import OutliersDataCheck\n",
    "\n",
    "outliers_check = OutliersDataCheck()\n",
    "\n",
    "for message in outliers_check.validate(X):\n",
    "    print (message.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DefaultDataChecks\n",
    "\n",
    "`AutoMLSearch.search` is able to run a set of data checks to ensure that the input data being passed will not run into some common issues before running a potentially time-consuming search. This is controlled by the `data_checks` parameter. By default, `data_checks` is set to `'auto'`, which will run the collection of data checks st\n",
    "We can also pass in our own list of data checks by 'none' or "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Data Checks\n",
    "\n",
    "By default, `AutoMLSearch.search` runs a collection of data checks before it searches and iterates over pipelines. This collection of data checks is stored in the `DefaultDataChecks` class. It consists of a few data checks that are generally helpful for any machine learning problem. They are:\n",
    "- `HighlyNullDataCheck`\n",
    "- `IDColumnsDataCheck`\n",
    "- `TargetLeakageDataCheck`\n",
    "- `InvalidTargetDataCheck`\n",
    "- `ClassImbalanceDataCheck`\n",
    "- `NoVarianceDataCheck`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Your Own Data Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would prefer to write your own data check, you can do so by extending the DataCheck class and implementing the `validate(self, X, y)` class method. Below, we've created a new DataCheck, `ZeroVarianceDataCheck`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from evalml.data_checks import DataCheck\n",
    "from evalml.data_checks.data_check_message import DataCheckError\n",
    "\n",
    "class ZeroVarianceDataCheck(DataCheck):\n",
    "    def validate(self, X, y):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        warning_msg = \"Column '{}' has zero variance\"\n",
    "        return [DataCheckError(warning_msg.format(column), self.name) for column in X.columns if len(X[column].unique()) == 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
