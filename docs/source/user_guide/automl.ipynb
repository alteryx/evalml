{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning (AutoML) Search\n",
    "\n",
    "## Background\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "[Machine learning](https://en.wikipedia.org/wiki/Machine_learning) (ML) is the process of constructing a mathematical model of a system based on a sample dataset collected from that system.\n",
    "\n",
    "One of the main goals of training an ML model is to teach the model to separate the signal present in the data from the noise inherent in system and in the data collection process. If this is done effectively, the model can then be used to make accurate predictions about the system when presented with new, similar data. Additionally, introspecting on an ML model can reveal key information about the system being modeled, such as which inputs and transformations of the inputs are most useful to the ML model for learning the signal in the data, and are therefore the most predictive.\n",
    "\n",
    "There are [a variety](https://en.wikipedia.org/wiki/Machine_learning#Approaches) of ML problem types. Supervised learning describes the case where the collected data contains an output value to be modeled and a set of inputs with which to train the model. EvalML focuses on training supervised learning models.\n",
    "\n",
    "EvalML supports three common supervised ML problem types. The first is regression, where the target value to model is a continuous numeric value. Next are binary and multiclass classification, where the target value to model consists of two or more discrete values or categories. The choice of which supervised ML problem type is most appropriate depends on domain expertise and on how the model will be evaluated and used.\n",
    "\n",
    "\n",
    "### AutoML and Search\n",
    "\n",
    "[AutoML](https://en.wikipedia.org/wiki/Automated_machine_learning) is the process of automating the construction, training and evaluation of ML models. Given a data and some configuration, AutoML searches for the most effective and accurate ML model or models to fit the dataset. During the search, AutoML will explore different combinations of model type, model parameters and model architecture.\n",
    "\n",
    "An effective AutoML solution offers several advantages over constructing and tuning ML models by hand. AutoML can assist with many of the difficult aspects of ML, such as avoiding overfitting and underfitting, imbalanced data, detecting data leakage and other potential issues with the problem setup, and automatically applying best-practice data cleaning, feature engineering, feature selection and various modeling techniques. AutoML can also leverage search algorithms to optimally sweep the hyperparameter search space, resulting in model performance which would be difficult to achieve by manual training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML in EvalML\n",
    "\n",
    "EvalML supports all of the above and more.\n",
    "\n",
    "In its simplest usage, the AutoML search interface requires only the input data, the target data and a `problem_type` specifying what kind of supervised ML problem to model.\n",
    "\n",
    "** Graphing methods, like AutoMLSearch, on Jupyter Notebook and Jupyter Lab require [ipywidgets](https://ipywidgets.readthedocs.io/en/latest/user_install.html) to be installed.\n",
    "\n",
    "** If graphing on Jupyter Lab, [jupyterlab-plotly](https://plotly.com/python/getting-started/#jupyterlab-support-python-35) required. To download this, make sure you have [npm](https://nodejs.org/en/download/) installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide data to EvalML, it is recommended that you create a `DataTable` object using [the Woodwork project](https://woodwork.alteryx.com/en/stable/).\n",
    "\n",
    "EvalML also accepts and works well with pandas `DataFrames`. But using the `DataTable` makes it easy to control how EvalML will treat each feature, as a numeric feature, a categorical feature, a text feature or other type of feature. Woodwork's `DataTable` includes features like inferring when a categorical feature should be treated as a text feature. For this reason, if you don't provide Woodwork objects, EvalML will raise a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-05 12:40:48,010 featuretools - WARNING    Featuretools failed to load plugin nlp_primitives from library nlp_primitives. For a full stack trace, set logging to debug.\n",
      "Using default limit of max_iterations=5.\n",
      "\n",
      "Generating pipelines to search over...\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Searching up to 5 pipelines. \n",
      "Allowed model families: decision_tree, random_forest, xgboost, catboost, linear_model, extra_trees, lightgbm\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7b1d8744244c51992d3f262fa5d967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Best Score',\n",
       "              'type'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: (1/5) Mode Baseline Binary Classification P... Elapsed:00:00\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 12.868\n",
      "2/5) Decision Tree Classifier w/ Imputer      Elapsed:00:00\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 1.965\n",
      "High coefficient of variation (cv >= 0.2) within cross validation scores. Decision Tree Classifier w/ Imputer may not perform as estimated on unseen data.\n",
      "3/5) LightGBM Classifier w/ Imputer           Elapsed:00:00\n",
      "\tStarting cross validation\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.130\n",
      "High coefficient of variation (cv >= 0.2) within cross validation scores. LightGBM Classifier w/ Imputer may not perform as estimated on unseen data.\n",
      "4/5) Extra Trees Classifier w/ Imputer        Elapsed:00:01\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.146\n",
      "5/5) Elastic Net Classifier w/ Imputer + S... Elapsed:00:03\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.504\n",
      "\n",
      "Search finished after 00:03            \n",
      "Best pipeline: LightGBM Classifier w/ Imputer\n",
      "Best pipeline Log Loss Binary: 0.130426\n"
     ]
    }
   ],
   "source": [
    "import evalml\n",
    "\n",
    "X, y = evalml.demos.load_breast_cancer()\n",
    "\n",
    "import woodwork as ww\n",
    "X_dt = ww.DataTable(X)\n",
    "y_dc = ww.DataColumn(y)\n",
    "\n",
    "automl = evalml.automl.AutoMLSearch(problem_type='binary')\n",
    "automl.search(X_dt, y_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoML search will log its progress, reporting each pipeline and parameter set evaluated during the search.\n",
    "\n",
    "There are a number of mechanisms to control the AutoML search time. One way is to set the maximum number of candidate models to be evaluated during AutoML using `max_iterations`. By default, AutoML will search a fixed number of iterations and parameter pairs (`max_iterations=5`). The first pipeline to be evaluated will always be a baseline model representing a trivial solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AutoML interface supports a variety of other parameters. For a comprehensive list, please [refer to the API reference.](../generated/evalml.automl.AutoMLSearch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Problem Type\n",
    "\n",
    "EvalML includes a simple method, `detect_problem_type`, to help determine the problem type given the target data. \n",
    "\n",
    "This function can return the predicted problem type as a ProblemType enum, choosing from ProblemType.BINARY, ProblemType.MULTICLASS, and ProblemType.REGRESSION. If the target data is invalid (for instance when there is only 1 unique label), the function will throw an error instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ProblemTypes.BINARY: 'binary'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evalml.problem_types import detect_problem_type\n",
    "\n",
    "y = pd.Series([0, 1, 1, 0, 1, 1])\n",
    "detect_problem_type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective parameter\n",
    "\n",
    "AutoMLSearch takes in an `objective` parameter to determine which `objective` to optimize for. By default, this parameter is set to `auto`, which allows AutoML to choose `LogLossBinary` for binary classification problems, `LogLossMulticlass` for multiclass classification problems, and `R2` for regression problems.\n",
    "\n",
    "It should be noted that the `objective` parameter is only used in ranking and helping choose the pipelines to iterate over, but is not used to optimize each individual pipeline during fit-time.\n",
    "\n",
    "To get the default objective for each problem type, you can use the `get_default_primary_search_objective` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss Binary\n",
      "Log Loss Multiclass\n",
      "R2\n"
     ]
    }
   ],
   "source": [
    "from evalml.automl import get_default_primary_search_objective\n",
    "\n",
    "binary_objective = get_default_primary_search_objective(\"binary\")\n",
    "multiclass_objective = get_default_primary_search_objective(\"multiclass\")\n",
    "regression_objective = get_default_primary_search_objective(\"regression\")\n",
    "\n",
    "print(binary_objective.name)\n",
    "print(multiclass_objective.name)\n",
    "print(regression_objective.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Checks\n",
    "\n",
    "`AutoMLSearch.search` runs a set of data checks before beginning the search process to ensure that the input data being passed will not run into some common issues before running a potentially time-consuming search. If the data checks find any potential errors, an exception will be thrown before the search begins, allowing users to inspect their data to avoid confusing errors that may arise later during the search process.\n",
    "\n",
    "This behavior is controlled by the `data_checks` parameter which can take in either a `DataChecks` object, a list of `DataCheck` objects, `None`, or valid string inputs (`\"disabled\"`, `\"auto\"`). By default, this parameter is set to `auto`, which runs the default collection of data sets defined in the `DefaultDataChecks` class. If set to `\"disabled\"` or None, no data checks will run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using custom pipelines\n",
    "\n",
    "EvalML's AutoML algorithm generates a set of pipelines to search with. To provide a custom set instead, set allowed_pipelines to a list of [custom pipeline](pipelines.ipynb) classes. Note: this will prevent AutoML from generating other pipelines to search over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default limit of max_iterations=5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evalml.pipelines import MulticlassClassificationPipeline\n",
    "\n",
    "class CustomMulticlassClassificationPipeline(MulticlassClassificationPipeline):\n",
    "    component_graph = ['Simple Imputer', 'Random Forest Classifier']\n",
    "\n",
    "automl_custom = evalml.automl.AutoMLSearch(problem_type='multiclass', allowed_pipelines=[CustomMulticlassClassificationPipeline])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping the search early\n",
    "\n",
    "To stop the search early, hit `Ctrl-C`. This will bring up a prompt asking for confirmation. Responding with `y` will immediately stop the search. Responding with `n` will continue the search.\n",
    "\n",
    "![Interrupting Search Demo](keyboard_interrupt_demo_updated.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Rankings\n",
    "A summary of all the pipelines built can be returned as a pandas DataFrame which is sorted by score. The score column contains the average score across all cross-validation folds while the validation_score column is computed from the first cross-validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>score</th>\n",
       "      <th>validation_score</th>\n",
       "      <th>percent_better_than_baseline</th>\n",
       "      <th>high_variance_cv</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>LightGBM Classifier w/ Imputer</td>\n",
       "      <td>0.130426</td>\n",
       "      <td>0.171504</td>\n",
       "      <td>98.986464</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Extra Trees Classifier w/ Imputer</td>\n",
       "      <td>0.146243</td>\n",
       "      <td>0.150788</td>\n",
       "      <td>98.863555</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Elastic Net Classifier w/ Imputer + Standard S...</td>\n",
       "      <td>0.504486</td>\n",
       "      <td>0.505864</td>\n",
       "      <td>96.079663</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Decision Tree Classifier w/ Imputer</td>\n",
       "      <td>1.965214</td>\n",
       "      <td>1.508027</td>\n",
       "      <td>84.728426</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Imputer': {'categorical_impute_strategy': 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Mode Baseline Binary Classification Pipeline</td>\n",
       "      <td>12.868443</td>\n",
       "      <td>12.906595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Baseline Classifier': {'strategy': 'mode'}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      pipeline_name      score  \\\n",
       "0   2                     LightGBM Classifier w/ Imputer   0.130426   \n",
       "1   3                  Extra Trees Classifier w/ Imputer   0.146243   \n",
       "2   4  Elastic Net Classifier w/ Imputer + Standard S...   0.504486   \n",
       "3   1                Decision Tree Classifier w/ Imputer   1.965214   \n",
       "4   0       Mode Baseline Binary Classification Pipeline  12.868443   \n",
       "\n",
       "   validation_score  percent_better_than_baseline  high_variance_cv  \\\n",
       "0          0.171504                     98.986464              True   \n",
       "1          0.150788                     98.863555             False   \n",
       "2          0.505864                     96.079663             False   \n",
       "3          1.508027                     84.728426              True   \n",
       "4         12.906595                      0.000000             False   \n",
       "\n",
       "                                          parameters  \n",
       "0  {'Imputer': {'categorical_impute_strategy': 'm...  \n",
       "1  {'Imputer': {'categorical_impute_strategy': 'm...  \n",
       "2  {'Imputer': {'categorical_impute_strategy': 'm...  \n",
       "3  {'Imputer': {'categorical_impute_strategy': 'm...  \n",
       "4      {'Baseline Classifier': {'strategy': 'mode'}}  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Pipeline\n",
    "Each pipeline is given an `id`. We can get more information about any particular pipeline using that `id`. Here, we will get more information about the pipeline with `id = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "* Decision Tree Classifier w/ Imputer *\n",
      "***************************************\n",
      "\n",
      "Problem Type: binary\n",
      "Model Family: Decision Tree\n",
      "\n",
      "Pipeline Steps\n",
      "==============\n",
      "1. Imputer\n",
      "\t * categorical_impute_strategy : most_frequent\n",
      "\t * numeric_impute_strategy : mean\n",
      "\t * categorical_fill_value : None\n",
      "\t * numeric_fill_value : None\n",
      "2. Decision Tree Classifier\n",
      "\t * criterion : gini\n",
      "\t * max_features : auto\n",
      "\t * max_depth : 6\n",
      "\t * min_samples_split : 2\n",
      "\t * min_weight_fraction_leaf : 0.0\n",
      "\n",
      "Training\n",
      "========\n",
      "Training for binary problems.\n",
      "Total training time (including CV): 0.2 seconds\n",
      "\n",
      "Cross Validation\n",
      "----------------\n",
      "             Log Loss Binary  MCC Binary   AUC  Precision    F1  Balanced Accuracy Binary  Accuracy Binary # Training # Testing\n",
      "0                      1.508       0.854 0.936      0.903 0.909                     0.928            0.932    379.000   190.000\n",
      "1                      2.547       0.843 0.910      0.901 0.901                     0.921            0.926    379.000   190.000\n",
      "2                      1.840       0.886 0.903      0.941 0.928                     0.940            0.947    380.000   189.000\n",
      "mean                   1.965       0.861 0.916      0.915 0.913                     0.930            0.935          -         -\n",
      "std                    0.531       0.023 0.018      0.023 0.013                     0.010            0.011          -         -\n",
      "coef of var            0.270       0.026 0.019      0.025 0.015                     0.010            0.012          -         -\n"
     ]
    }
   ],
   "source": [
    "automl.describe_pipeline(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pipeline\n",
    "We can get the object of any pipeline via their `id` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier w/ Imputer\n",
      "{'Imputer': {'categorical_impute_strategy': 'most_frequent', 'numeric_impute_strategy': 'mean', 'categorical_fill_value': None, 'numeric_fill_value': None}, 'Decision Tree Classifier': {'criterion': 'gini', 'max_features': 'auto', 'max_depth': 6, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "pipeline = automl.get_pipeline(1)\n",
    "print(pipeline.name)\n",
    "print(pipeline.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best pipeline\n",
    "If we specifically want to get the best pipeline, there is a convenient accessor for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Classifier w/ Imputer\n",
      "{'Imputer': {'categorical_impute_strategy': 'most_frequent', 'numeric_impute_strategy': 'mean', 'categorical_fill_value': None, 'numeric_fill_value': None}, 'LightGBM Classifier': {'boosting_type': 'gbdt', 'learning_rate': 0.1, 'n_estimators': 100, 'max_depth': 0, 'num_leaves': 31, 'min_child_samples': 20, 'n_jobs': -1, 'bagging_freq': 0, 'bagging_fraction': 0.9}}\n"
     ]
    }
   ],
   "source": [
    "best_pipeline = automl.best_pipeline\n",
    "print(best_pipeline.name)\n",
    "print(best_pipeline.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limiting the AutoML Search Space\n",
    "The AutoML search algorithm first trains each component in the pipeline with their default values. After the first iteration, it then tweaks the parameters of these components using the pre-defined hyperparameter ranges that these components have. To limit the search over certain hyperparameter ranges, use `make_pipeline` to define a pipeline with a custom hyperparameter range. Hyperparameter ranges can be found through the [API reference](https://evalml.alteryx.com/en/stable/api_reference.html) for each component. Note: The default value of the component must be included in any specified hyperparameter range for AutoMLSearch to succeed. Additionally, the parameter value must be specified as a list, even for just one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Number of Features\n",
      "Boolean                       1\n",
      "Categorical                   6\n",
      "Numeric                       5\n",
      "\n",
      "Number of training examples: 99992\n",
      "Targets\n",
      "False    84.82%\n",
      "True     15.18%\n",
      "Name: fraud, dtype: object\n",
      "Using default limit of max_iterations=5.\n",
      "\n",
      "`X` passed was not a DataTable. EvalML will try to convert the input as a Woodwork DataTable and types will be inferred. To control this behavior, please pass in a Woodwork DataTable instead.\n",
      "`y` passed was not a DataColumn. EvalML will try to convert the input as a Woodwork DataTable and types will be inferred. To control this behavior, please pass in a Woodwork DataTable instead.\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Searching up to 5 pipelines. \n",
      "Allowed model families: extra_trees\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1c3f632e8f4887b7b261c19eed9fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Best Score',\n",
       "              'type'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: (1/5) Mode Baseline Binary Classification P... Elapsed:00:00\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 5.243\n",
      "2/5) Extra Trees Classifier w/ Imputer + D... Elapsed:00:00\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.357\n",
      "3/5) Extra Trees Classifier w/ Imputer + D... Elapsed:00:23\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.378\n",
      "4/5) Extra Trees Classifier w/ Imputer + D... Elapsed:00:47\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.353\n",
      "5/5) Extra Trees Classifier w/ Imputer + D... Elapsed:01:26\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.337\n",
      "\n",
      "Search finished after 01:57            \n",
      "Best pipeline: Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder\n",
      "Best pipeline Log Loss Binary: 0.337176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Imputer': {'categorical_impute_strategy': ['most_frequent'],\n",
       "  'numeric_impute_strategy': ['mean', 'median']},\n",
       " 'DateTime Featurization Component': {},\n",
       " 'One Hot Encoder': {},\n",
       " 'Extra Trees Classifier': {'n_estimators': Integer(low=10, high=1000, prior='uniform', transform='identity'),\n",
       "  'max_features': ['auto', 'sqrt', 'log2'],\n",
       "  'max_depth': Integer(low=4, high=10, prior='uniform', transform='identity')}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evalml import AutoMLSearch\n",
    "from evalml.demos import load_fraud\n",
    "from evalml.pipelines.components.utils import get_estimators\n",
    "from evalml.model_family import ModelFamily\n",
    "from evalml.pipelines.utils import make_pipeline\n",
    "import woodwork as ww\n",
    "\n",
    "X, y = load_fraud()\n",
    "\n",
    "# example of setting parameter to just one value\n",
    "custom_hyperparameters = {'Imputer': {\n",
    "    'numeric_impute_strategy': ['mean']\n",
    "}}\n",
    "\n",
    "# limit the numeric impute strategy to include only `mean` and `median`\n",
    "# `mean` is the default value for this argument, and it needs to be included in the specified hyperparameter range\n",
    "custom_hyperparameters = {'Imputer': {\n",
    "    'numeric_impute_strategy': ['mean', 'median']\n",
    "}}\n",
    "\n",
    "# AutoMLSearch uses woodwork, so we want to use ww to convert the appropriate types when making pipelines\n",
    "# ww changes the original X and y data, so we pass that instead of X_dt, y_dc\n",
    "X_dt = ww.DataTable(X)\n",
    "y_dc = ww.DataColumn(y)\n",
    "\n",
    "estimators = get_estimators('binary', [ModelFamily.EXTRA_TREES])\n",
    "pipelines_with_custom_hyperparameters = [make_pipeline(X, y, estimator, 'binary', custom_hyperparameters) for estimator in estimators]\n",
    "automl = AutoMLSearch(problem_type='binary', allowed_pipelines=pipelines_with_custom_hyperparameters)\n",
    "automl.search(X, y)\n",
    "automl.best_pipeline.hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access raw results\n",
    "\n",
    "The `AutoMLSearch` class records detailed results information under the `results` field, including information about the cross-validation scoring and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipeline_results': {0: {'id': 0,\n",
       "   'pipeline_name': 'Mode Baseline Binary Classification Pipeline',\n",
       "   'pipeline_class': evalml.pipelines.classification.baseline_binary.ModeBaselineBinaryPipeline,\n",
       "   'pipeline_summary': 'Baseline Classifier',\n",
       "   'parameters': {'Baseline Classifier': {'strategy': 'mode'}},\n",
       "   'score': 5.243060307948459,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 0.9124500751495361,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   5.243353291477845),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.5),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.848189373256128),\n",
       "                  ('# Training', 66661),\n",
       "                  ('# Testing', 33331)]),\n",
       "     'score': 5.243353291477845,\n",
       "     'binary_classification_threshold': 0.5},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   5.243353291477846),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.5),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.848189373256128),\n",
       "                  ('# Training', 66661),\n",
       "                  ('# Testing', 33331)]),\n",
       "     'score': 5.243353291477846,\n",
       "     'binary_classification_threshold': 0.5},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   5.242474340889684),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.5),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.8482148214821482),\n",
       "                  ('# Training', 66662),\n",
       "                  ('# Testing', 33330)]),\n",
       "     'score': 5.242474340889684,\n",
       "     'binary_classification_threshold': 0.5}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 0,\n",
       "    'MCC Binary': nan,\n",
       "    'AUC': 0,\n",
       "    'Precision': nan,\n",
       "    'F1': nan,\n",
       "    'Balanced Accuracy Binary': 0,\n",
       "    'Accuracy Binary': 0},\n",
       "   'percent_better_than_baseline': 0,\n",
       "   'validation_score': 5.243353291477845},\n",
       "  1: {'id': 1,\n",
       "   'pipeline_name': 'Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder',\n",
       "   'pipeline_class': evalml.pipelines.utils.make_pipeline.<locals>.GeneratedPipeline,\n",
       "   'pipeline_summary': 'Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'mean',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour']},\n",
       "    'One Hot Encoder': {'top_n': 10,\n",
       "     'features_to_encode': None,\n",
       "     'categories': None,\n",
       "     'drop': None,\n",
       "     'handle_unknown': 'ignore',\n",
       "     'handle_missing': 'error'},\n",
       "    'Extra Trees Classifier': {'n_estimators': 100,\n",
       "     'max_features': 'auto',\n",
       "     'max_depth': 6,\n",
       "     'min_samples_split': 2,\n",
       "     'min_weight_fraction_leaf': 0.0,\n",
       "     'n_jobs': -1}},\n",
       "   'score': 0.35725193009339407,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 22.94388484954834,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.3614971864773663),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.8355950447413045),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.848189373256128),\n",
       "                  ('# Training', 66661),\n",
       "                  ('# Testing', 33331)]),\n",
       "     'score': 0.3614971864773663,\n",
       "     'binary_classification_threshold': 0.5},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.36202153389616865),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.8325158408251699),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.848189373256128),\n",
       "                  ('# Training', 66661),\n",
       "                  ('# Testing', 33331)]),\n",
       "     'score': 0.36202153389616865,\n",
       "     'binary_classification_threshold': 0.5},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.3482370699066474),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.8377868329964773),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.8482148214821482),\n",
       "                  ('# Training', 66662),\n",
       "                  ('# Testing', 33330)]),\n",
       "     'score': 0.3482370699066474,\n",
       "     'binary_classification_threshold': 0.5}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 93.18619452933238,\n",
       "    'MCC Binary': nan,\n",
       "    'AUC': 67.05984790419677,\n",
       "    'Precision': nan,\n",
       "    'F1': nan,\n",
       "    'Balanced Accuracy Binary': 0,\n",
       "    'Accuracy Binary': 0},\n",
       "   'percent_better_than_baseline': 93.18619452933238,\n",
       "   'validation_score': 0.3614971864773663},\n",
       "  2: {'id': 2,\n",
       "   'pipeline_name': 'Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder',\n",
       "   'pipeline_class': evalml.pipelines.utils.make_pipeline.<locals>.GeneratedPipeline,\n",
       "   'pipeline_summary': 'Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'mean',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour']},\n",
       "    'One Hot Encoder': {'top_n': 10,\n",
       "     'features_to_encode': None,\n",
       "     'categories': None,\n",
       "     'drop': None,\n",
       "     'handle_unknown': 'ignore',\n",
       "     'handle_missing': 'error'},\n",
       "    'Extra Trees Classifier': {'n_estimators': 158,\n",
       "     'max_features': 'sqrt',\n",
       "     'max_depth': 4,\n",
       "     'min_samples_split': 2,\n",
       "     'min_weight_fraction_leaf': 0.0,\n",
       "     'n_jobs': -1}},\n",
       "   'score': 0.3781705591796319,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 23.624637842178345,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.38356129376972303),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.834654752429304),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.848189373256128),\n",
       "                  ('# Training', 66661),\n",
       "                  ('# Testing', 33331)]),\n",
       "     'score': 0.38356129376972303,\n",
       "     'binary_classification_threshold': 0.5},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.3767592940207891),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.832534634787558),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.848189373256128),\n",
       "                  ('# Training', 66661),\n",
       "                  ('# Testing', 33331)]),\n",
       "     'score': 0.3767592940207891,\n",
       "     'binary_classification_threshold': 0.5},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.3741910897483836),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.8383302805956601),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.8482148214821482),\n",
       "                  ('# Training', 66662),\n",
       "                  ('# Testing', 33330)]),\n",
       "     'score': 0.3741910897483836,\n",
       "     'binary_classification_threshold': 0.5}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 92.78721706469165,\n",
       "    'MCC Binary': nan,\n",
       "    'AUC': 67.0346445208348,\n",
       "    'Precision': nan,\n",
       "    'F1': nan,\n",
       "    'Balanced Accuracy Binary': 0,\n",
       "    'Accuracy Binary': 0},\n",
       "   'percent_better_than_baseline': 92.78721706469165,\n",
       "   'validation_score': 0.38356129376972303},\n",
       "  3: {'id': 3,\n",
       "   'pipeline_name': 'Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder',\n",
       "   'pipeline_class': evalml.pipelines.utils.make_pipeline.<locals>.GeneratedPipeline,\n",
       "   'pipeline_summary': 'Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'median',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour']},\n",
       "    'One Hot Encoder': {'top_n': 10,\n",
       "     'features_to_encode': None,\n",
       "     'categories': None,\n",
       "     'drop': None,\n",
       "     'handle_unknown': 'ignore',\n",
       "     'handle_missing': 'error'},\n",
       "    'Extra Trees Classifier': {'n_estimators': 885,\n",
       "     'max_features': 'auto',\n",
       "     'max_depth': 7,\n",
       "     'min_samples_split': 2,\n",
       "     'min_weight_fraction_leaf': 0.0,\n",
       "     'n_jobs': -1}},\n",
       "   'score': 0.3527196364539062,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 38.9995481967926,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.3524316291380676),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.8344184734898525),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.848189373256128),\n",
       "                  ('# Training', 66661),\n",
       "                  ('# Testing', 33331)]),\n",
       "     'score': 0.3524316291380676,\n",
       "     'binary_classification_threshold': 0.5},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.3528537186053568),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.8342668635005381),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.848189373256128),\n",
       "                  ('# Training', 66661),\n",
       "                  ('# Testing', 33331)]),\n",
       "     'score': 0.3528537186053568,\n",
       "     'binary_classification_threshold': 0.5},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.3528735616182942),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.8398654079310286),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.8482148214821482),\n",
       "                  ('# Training', 66662),\n",
       "                  ('# Testing', 33330)]),\n",
       "     'score': 0.3528735616182942,\n",
       "     'binary_classification_threshold': 0.5}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 93.272638197215,\n",
       "    'MCC Binary': nan,\n",
       "    'AUC': 67.23671632809463,\n",
       "    'Precision': nan,\n",
       "    'F1': nan,\n",
       "    'Balanced Accuracy Binary': 0,\n",
       "    'Accuracy Binary': 0},\n",
       "   'percent_better_than_baseline': 93.272638197215,\n",
       "   'validation_score': 0.3524316291380676},\n",
       "  4: {'id': 4,\n",
       "   'pipeline_name': 'Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder',\n",
       "   'pipeline_class': evalml.pipelines.utils.make_pipeline.<locals>.GeneratedPipeline,\n",
       "   'pipeline_summary': 'Extra Trees Classifier w/ Imputer + DateTime Featurization Component + One Hot Encoder',\n",
       "   'parameters': {'Imputer': {'categorical_impute_strategy': 'most_frequent',\n",
       "     'numeric_impute_strategy': 'mean',\n",
       "     'categorical_fill_value': None,\n",
       "     'numeric_fill_value': None},\n",
       "    'DateTime Featurization Component': {'features_to_extract': ['year',\n",
       "      'month',\n",
       "      'day_of_week',\n",
       "      'hour']},\n",
       "    'One Hot Encoder': {'top_n': 10,\n",
       "     'features_to_encode': None,\n",
       "     'categories': None,\n",
       "     'drop': None,\n",
       "     'handle_unknown': 'ignore',\n",
       "     'handle_missing': 'error'},\n",
       "    'Extra Trees Classifier': {'n_estimators': 342,\n",
       "     'max_features': 'auto',\n",
       "     'max_depth': 10,\n",
       "     'min_samples_split': 2,\n",
       "     'min_weight_fraction_leaf': 0.0,\n",
       "     'n_jobs': -1}},\n",
       "   'score': 0.33717554404111055,\n",
       "   'high_variance_cv': False,\n",
       "   'training_time': 31.04630184173584,\n",
       "   'cv_data': [{'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.3357808712707505),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.8416592485798446),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.848189373256128),\n",
       "                  ('# Training', 66661),\n",
       "                  ('# Testing', 33331)]),\n",
       "     'score': 0.3357808712707505,\n",
       "     'binary_classification_threshold': 0.5},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.33896418383904714),\n",
       "                  ('MCC Binary', 0.01294725405808866),\n",
       "                  ('AUC', 0.8324434611760847),\n",
       "                  ('Precision', 1.0),\n",
       "                  ('F1', 0.0003951788184153329),\n",
       "                  ('Balanced Accuracy Binary', 0.5000988142292491),\n",
       "                  ('Accuracy Binary', 0.8482193753562749),\n",
       "                  ('# Training', 66661),\n",
       "                  ('# Testing', 33331)]),\n",
       "     'score': 0.33896418383904714,\n",
       "     'binary_classification_threshold': 0.5},\n",
       "    {'all_objective_scores': OrderedDict([('Log Loss Binary',\n",
       "                   0.336781577013534),\n",
       "                  ('MCC Binary', 0.0),\n",
       "                  ('AUC', 0.8352236786213438),\n",
       "                  ('Precision', 0.0),\n",
       "                  ('F1', 0.0),\n",
       "                  ('Balanced Accuracy Binary', 0.5),\n",
       "                  ('Accuracy Binary', 0.8482148214821482),\n",
       "                  ('# Training', 66662),\n",
       "                  ('# Testing', 33330)]),\n",
       "     'score': 0.336781577013534,\n",
       "     'binary_classification_threshold': 0.5}],\n",
       "   'percent_better_than_baseline_all_objectives': {'Log Loss Binary': 93.56910803543585,\n",
       "    'MCC Binary': nan,\n",
       "    'AUC': 67.2884258918182,\n",
       "    'Precision': nan,\n",
       "    'F1': nan,\n",
       "    'Balanced Accuracy Binary': 0.00658761528327112,\n",
       "    'Accuracy Binary': 0.0011790527384916232},\n",
       "   'percent_better_than_baseline': 93.56910803543585,\n",
       "   'validation_score': 0.3357808712707505}},\n",
       " 'search_order': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
